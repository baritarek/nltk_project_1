{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "In the following file is meant to retireve the following reuters21578 files and from there it will clean , tokenize and remove the stop words while outputing a cleaner version of the data.\n",
    "\n",
    "Thus using NLTK in main.py I will be implementing a pipeline that will:\n",
    "    1. Read the Reuter's collection and extract raw text\n",
    "    2. Tokenize the raw text extracted\n",
    "    3. Lower case the raw text extracted\n",
    "    4. Apply the Porter stemmer for the raw text\n",
    "    5. Remove the stop words from the text\n",
    "\n",
    "In the following documentation I will provide the explanation for the packages and functions used in order to achieve the solutions indicated above (The 5 steps)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The packages used in the following project are nltk , re ,os and nltk.corpus stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "import pprint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5 fiels to process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# To process the first five files\n",
    "files_to_process = ['reut2-000.sgm', 'reut2-001.sgm', 'reut2-002.sgm', 'reut2-003.sgm', 'reut2-004.sgm']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "function 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will be used to remove any lines before and after the html tag <BODY></BODY>\n",
    "@:param lines will be used to read the lines of the .sgm files\n",
    "\"\"\"\n",
    "def get_pure_sentences(lines):\n",
    "    valid_lines = []\n",
    "    not_done = True\n",
    "    i = 0\n",
    "    while not_done:\n",
    "        match = re.search(r'.*<BODY>', lines[i])\n",
    "        if match:\n",
    "            first_line = True\n",
    "            not_end = True\n",
    "            while not_done and not_end:\n",
    "                matchb = re.search(r'.*</BODY>', lines[i])\n",
    "                if matchb:\n",
    "                    not_end = False\n",
    "                else:\n",
    "                    if first_line:\n",
    "                        sentence = re.sub(r'.*<BODY>', \"\", lines[i])\n",
    "                        valid_lines.append(sentence)\n",
    "                    else:\n",
    "                        valid_lines.append(lines[i])\n",
    "                i += 1\n",
    "                if i == len(lines):\n",
    "                    not_done = False\n",
    "        else:\n",
    "            if not_done:\n",
    "                i += 1\n",
    "                if i == len(lines):\n",
    "                    not_done = False\n",
    "    return valid_lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to read and clean the filenames while having a latin-1 encoded\n",
    "@:param filename used for the .sgm file names\n",
    "@:param path used to set the directory\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_doc(filename, path):\n",
    "    filename = path + '/' + filename\n",
    "    with open(filename, encoding='latin-1') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "    return lines\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Third function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function is used to tokenize each word found in the files using the ntlk.word_tokenize\n",
    "@:param lines used to read the lines of the files and tokenize each word\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenizer(lines):\n",
    "    words = []\n",
    "    for res in lines:\n",
    "        word = nltk.word_tokenize(res)\n",
    "        for w in word:\n",
    "            words.append(w)\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fourth function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This function is meant to lower case all the words in the files\n",
    "@:param words used to store the the words of the file in order to lowercase\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def lowercase(words):\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].lower()\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5th function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function uses the porter stemmer of nltk while stemming all words\n",
    "@:param used to store the words in nltk.PorterStemmer().stem()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def stemmer(words):\n",
    "    for i in range(len(words)):\n",
    "        words[i] = nltk.PorterStemmer().stem(words[i])\n",
    "    return words\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6th function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to remove a list of stop words from the list\n",
    "@:param words will be used to filter the words in files\n",
    "@:param stop_words is used to accept words as a list\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def filter_stop_words(words, stop_words=sw.words('english')):\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    return filtered_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MAINNNNNNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files_to_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# if f in files_to_process\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# loop files directory using os.listidir()\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# if re.match(r'.*\\.sgm', f)\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     files \u001B[38;5;241m=\u001B[39m [f \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreuters21578\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m files_to_process]\n\u001B[1;32m      7\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m files:\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# calling read_doc function to read the project directory\u001B[39;00m\n",
      "Cell \u001B[0;32mIn [2], line 5\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# if f in files_to_process\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# loop files directory using os.listidir()\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# if re.match(r'.*\\.sgm', f)\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     files \u001B[38;5;241m=\u001B[39m [f \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreuters21578\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[43mfiles_to_process\u001B[49m]\n\u001B[1;32m      7\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m files:\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# calling read_doc function to read the project directory\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'files_to_process' is not defined"
     ]
    }
   ],
   "source": [
    "# if f in files_to_process\n",
    "if __name__ == \"__main__\":\n",
    "    # loop files directory using os.listidir()\n",
    "    # if re.match(r'.*\\.sgm', f)\n",
    "    files = [f for f in os.listdir('reuters21578') if f in files_to_process]\n",
    "\n",
    "    sentences = []\n",
    "    for file in files:\n",
    "        # calling read_doc function to read the project directory\n",
    "        raw = read_doc(file, 'reuters21578')\n",
    "        # get_pure_sentences to retrieve the temporary senteces in reuters\n",
    "        temp_sentences = get_pure_sentences(raw)\n",
    "        # merging the files into one array rather than a multidimensional\n",
    "        sentences.extend(temp_sentences)\n",
    "\n",
    "    # tokenize each words\n",
    "    words = tokenizer(sentences)\n",
    "\n",
    "    # meant to lower case the words tokenized\n",
    "    lower_case_words = lowercase(words)\n",
    "\n",
    "    # using the Porter stemmer to stem the lower_case_words\n",
    "    stemmer_words = stemmer(lower_case_words)\n",
    "\n",
    "    # filter the stop_words\n",
    "    filter_words = filter_stop_words(stemmer_words)\n",
    "\n",
    "    # pretty print the filter_words\n",
    "    pprint.pp(filter_words)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
